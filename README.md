# ğŸŒŒ AURA : Offline AI Vision Assistant

> â€œWhere your gestures meet AI intuition.â€

AURA is an **offline AI-powered gesture and vision-based assistant** built with **Python**, **Mediapipe**, and **YOLOv8**.  
It detects hand gestures in real-time and performs automated actions â€” all while recognizing real-world objects using computer vision.  
Designed to be futuristic yet lightweight, AURA+ redefines how humans interact with machines.

---

## ğŸ§  Overview

AURA enables **natural human-computer interaction** through camera input.  
It combines gesture control and object detection into one intelligent, offline system.

**Modes:**
- ğŸ’¤ **Passive Mode:** Gesture-based quick controls (e.g., open app, take screenshot).
- ğŸš€ **Active Mode:** Full AI engagement â€” detects both gestures and objects with voice feedback.

---

## âš™ï¸ Features

- âœ‹ **Gesture Recognition** using Mediapipe  
- ğŸ¯ **Object Detection** via YOLOv8 (pre-trained model)  
- ğŸ§© **Offline Processing** â€” No internet or cloud dependency  
- ğŸ”Š **Voice Feedback** powered by Pyttsx3  
- ğŸª¶ **Lightweight and Modular** Python architecture  
- ğŸ’» **Cross-Platform:** Works on Windows, Linux, macOS  

---

## ğŸ§© Project Structure
```

AURA/
â”‚
â”œâ”€â”€ main.py                     # Main execution file
â”œâ”€â”€ gestures/
â”‚   â”œâ”€â”€ gesture_controller.py   # Handles gesture recognition and mapping
â”‚
â”œâ”€â”€ vision/
â”‚   â”œâ”€â”€ yolo_detector.py        # YOLOv8-based object detection
â”‚   â”œâ”€â”€ model/                  # YOLO weights (e.g., yolov8n.pt)
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ voice_feedback.py       # Voice responses (optional)
â”‚   â”œâ”€â”€ hud_overlay.py          # On-screen info display (optional)
â”‚
â”œâ”€â”€ assets/                     # Icons, sounds, visual elements
â”‚
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # Project documentation

````

---

## ğŸ§  How It Works

1. **Camera Activation:** Captures video frames using OpenCV.  
2. **Gesture Detection:** Mediapipe analyzes hand landmarks and classifies gestures.  
3. **Action Mapping:** Each gesture is linked to a system function or mode switch.  
4. **Object Detection:** YOLOv8 identifies real-world objects from live feed.  
5. **Voice Feedback:** AURA+ announces gestures or detected items via offline TTS.  
6. **Mode Control:** Gestures like *open palm* toggle between passive and active modes.

---

## ğŸ§° Tech Stack

| Component | Technology |
|------------|-------------|
| Gesture Recognition | Mediapipe |
| Object Detection | YOLOv8 (Ultralytics) |
| Programming Language | Python 3.x |
| Frameworks | OpenCV, PyTorch |
| Voice System | Pyttsx3 |
| Interface | CLI + OpenCV HUD |

---

## ğŸ“¦ Installation

### 1. Clone the Repository
```bash
git clone https://github.com/ayush-thakur1107/AURA.git
cd AURA
````

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Run the Application

```bash
python main.py
```

---

## ğŸ§  Future Enhancements

* Integrate face recognition for personalized mode switching
* Add emotion detection (using CNN models)
* Build a GUI dashboard using PyQt
* Enable offline speech recognition (for two-way conversation)

---


## ğŸ§¾ License

This project is licensed under the MIT License â€” free to use, modify, and build upon with credit.

---

## ğŸ‘¨â€ğŸ’» Author

**Ayush Thakur**
AI & Computer Vision Enthusiast
GitHub: [@ayush-thakur1107](https://github.com/ayush-thakur1107)
Project: [AURA](https://github.com/ayush-thakur1107/AURA)

---

> â€œAURA isnâ€™t just a project â€” itâ€™s a glimpse into the touchless future.â€


